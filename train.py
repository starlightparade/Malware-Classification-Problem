##get data from csv files
import numpy as np
import pandas as pd

import keras
from keras.models import Sequential
from keras.layers import Dense, Dropout
from keras.layers import Embedding, BatchNormalization,GaussianNoise
from keras.layers import Conv1D, GlobalMaxPooling1D, MaxPooling1D, AveragePooling1D, Activation
from keras import optimizers, regularizers
from keras.callbacks import ReduceLROnPlateau, LearningRateScheduler, EarlyStopping
import xgboost as xgb
import matplotlib.pyplot as plt

#############
####importing files from google drive
#!pip install -U -q PyDrive
#from pydrive.auth import GoogleAuth
#from pydrive.drive import GoogleDrive
#from google.colab import auth
#from oauth2client.client import GoogleCredentials

# 1. Authenticate and create the PyDrive client.
#auth.authenticate_user()
#gauth = GoogleAuth()
#gauth.credentials = GoogleCredentials.get_application_default()
#drive = GoogleDrive(gauth)

#2. Get the file
#downloaded = drive.CreateFile({'id':'1Ca3N5KRqsp41559Wwf2tEo4uAEwxH0SB'}) 
#downloaded.GetContentFile('train.csv')  

#downloaded = drive.CreateFile({'id':'15rXa0KJ96QHc4Xgf6GMvn8queqFdeLUG'})
#downloaded.GetContentFile('train_label.csv') 

#downloaded = drive.CreateFile({'id':'1g24em7VpCX9FYG3ELD3MwpyiXrivmu3Q'})
#downloaded.GetContentFile('test.csv')

class Dataset():
    def __init__(self):
        self.x_train = None
        self.y_train = None
        self.x_val = None
        self.y_val = None
        self.x_test = None
        self.y_test = None
        self._load_data()

    def _load_data(self):
        data = pd.read_csv('train.csv', sep='/t', header=None, engine='python')
        data_label = pd.read_csv('train_label.csv', sep=',', header=None, engine='python')
        data_test = pd.read_csv('test.csv', sep='/t', header=None, engine='python')
        #print(data)
        #print(data_label)
        max_size = 1024
        x = []
        y = []
        test = []
        for i in range(len(data.values)):
            temp_data = data.values[i][0]
            temp_list = temp_data.split(',')   
            temp_array = np.asarray(temp_list[0:max_size], dtype=np.int32)
            if(temp_array.shape[0]<max_size):
                a = np.zeros((max_size,))
                a[0:temp_array.shape[0]] = temp_array
                temp_array = a
            x.append(temp_array)
        for i in range(len(data_test.values)):
            temp_data = data_test.values[i][0]
            temp_list = temp_data.split(',')   
            temp_array = np.asarray(temp_list[0:max_size], dtype=np.int32)
            if(temp_array.shape[0]<max_size):
                a = np.zeros((max_size,))
                a[0:temp_array.shape[0]] = temp_array
                temp_array = a
            test.append(temp_array)
        for i in range(len(data_label.values)):
            if i is not 0:
                y.append(data_label.values[i][1])
        x = np.asarray(x, dtype=np.int32)
        y = np.asarray(y, dtype=np.int32)
        test = np.asarray(test, dtype=np.int32)
        #print("x:",x.shape)
        #print("y:",y.shape)
        #print("len(x):",len(x))

        indices = np.arange(len(x))
        np.random.shuffle(indices)
        val_size = int(len(x)*0.2)
        self.x_val = x[indices[: val_size]]
        self.y_val = y[indices[: val_size]]

        self.x_train = x[indices[val_size :]]
        self.y_train = y[indices[val_size :]]
        self.x_test = test
    
    
#create model and train
  
def create_cnn_model():
    dataset = Dataset()
    x_train = dataset.x_train
    length = len(x_train)
    y_train = dataset.y_train
    x_val = dataset.x_val
    y_val = dataset.y_val
    x_test = dataset.x_test
    batch,timesteps = x_train.shape[0],x_train.shape[1]
    model = keras.Sequential()

    print("x shape:",x_train.shape)
    learning_rate_reduction = ReduceLROnPlateau(monitor='val_loss', 
                                                patience=2, 
                                                verbose=1, 
                                                factor=0.5, 
                                                min_lr=0.00001)
    early_stop = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=5, verbose=0, mode='auto')
    keras_model = Sequential()
    keras_model.add(Embedding(x_train.shape[0],100, input_length = timesteps))
    keras_model.add(Dropout(0.1))
    keras_model.add(Conv1D(256, 3,padding='valid',strides=1))
    keras_model.add(BatchNormalization())
    keras_model.add(Activation('relu'))
    keras_model.add(GlobalMaxPooling1D())
    keras_model.add(Dropout(0.2))

    keras_model.add(Dense(256))
    keras_model.add(BatchNormalization())
    keras_model.add(Activation('relu'))
    keras_model.add(Dropout(0.5))
    keras_model.add(Dense(1))
    keras_model.add(Activation('sigmoid'))

    keras_model.summary()
    keras_model.compile(loss='binary_crossentropy',
                      optimizer=optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False),
                      metrics=['accuracy'])

    history = keras_model.fit(x_train, y_train, batch_size=64, epochs=30, validation_data=(x_val, y_val),
              verbose = 1, callbacks=[learning_rate_reduction,early_stop])
    
    # summarize history for acc
    print(history.history.keys())
    plt.plot(history.history['acc'])
    plt.title('Training Accuracy')
    plt.ylabel('accuracy')
    plt.xlabel('epoch')
    plt.legend(['train'], loc='upper left')
    plt.show()
    plt.plot(history.history['val_acc'])
    plt.title('Validation Accuracy')
    plt.ylabel('accuracy')
    plt.xlabel('epoch')
    plt.legend(['test'], loc='upper left')
    plt.show()
    # summarize history for loss
    plt.plot(history.history['loss'])
    plt.title('Training Loss')
    plt.ylabel('loss')
    plt.xlabel('epoch')
    plt.legend(['train'], loc='upper left')
    plt.show()
    plt.plot(history.history['val_loss'])
    plt.title('Validation Loss')
    plt.ylabel('loss')
    plt.xlabel('epoch')
    plt.legend(['test'], loc='upper left')
    plt.show()
    print("keras model returned")
    return keras_model

def create_xgb_model():
    dataset = Dataset()
    x_train = dataset.x_train
    length = len(x_train)
    y_train = dataset.y_train
    x_val = dataset.x_val
    y_val = dataset.y_val
    x_test = dataset.x_test

    dtrain = xgb.DMatrix(x_train, label=y_train)
    dval = xgb.DMatrix(x_val, label=y_val)
    params = {
        'max_depth':8,
        'min_child_weight': 4,
        'eta':.3,
        'subsample': 0.8,
        'colsample_bytree': 0.8,
        'objective':'binary:logistic',
      }
    params['eval_metric'] = "error"

    xgb_model = xgb.train(
            params,
            dtrain,
            num_boost_round=300,
            evals=[(dval, "Test")],
            early_stopping_rounds=20
    )


    print("Best MAE: {:.2f} with {} rounds".format(
                     xgb_model.best_score,
                     xgb_model.best_iteration+1))
    return xgb_model
  